{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de documentos **WORD2VEC**\n",
    "Cundo se quiere realizar una clasificación de documentos mediante Word2Vec el primer paso es la construcción del modelo y con ello la del **Corpus de entrenamiento**\n",
    "\n",
    "### Eliminación de StopWords y caracteres especiales:\n",
    "Comenzaremos utilizando un archivo llamado *stop_words.txt* en el cual se tiene una lista de palabras que no aportan valor al corpus de entrenamiento por lo que son desechadas. Igualmente removemos todos los caracteres no alfanuméricos y los dígitos para contener unicamente las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "import re\n",
    "import nltk\n",
    "from nltk import *\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "\n",
    "LISTA_NO_ALFANUMERICOS = [\"!\", \"@\", \"#\", \"$\", \"%\", \"&\", \"*\", \"(\", \")\", \"-\", \"\", \"+\", \"=\", \"{\", \"}\", \"[\", \"]\", \"|\", \"'\", \"\\\\\", \";\", \":\", \",\", \".\", \"<\", \">\", \"/\", \"?\", \"¡\", \"¿\", \"¨\", \"´\", \"¨\", \"~\", \"¬\", \"`\", \"^\", \"¦\", \"ª\", \"º\", \"«\", \"»\", \"©\", \"®\", \"™\", \"§\", \"±\", \"µ\", \"÷\", \"×\", \"``\", \"\\\\d\", \"-\"]\n",
    "\n",
    "def tokenizar(archivo):\n",
    "    with open(archivo, \"r\", encoding=\"utf-8\") as file:\n",
    "        texto = file.read()\n",
    "\n",
    "    # Tokenizar el texto en frases\n",
    "    frases = re.sub(\"\\\\d\", \"\", unidecode(texto))\n",
    "    frases = re.sub(\"\\\\.\", \"\", frases)\n",
    "    frases = nltk.sent_tokenize(frases)\n",
    "\n",
    "    # Tokenizar cada frase en palabras\n",
    "    corpus = [word_tokenize(frase) for frase in frases]\n",
    "\n",
    "    stop_words = open(\"stop_words.txt\", encoding='utf-8')\n",
    "    stop_words = stop_words.read()\n",
    "    stop_words = unidecode(stop_words).split(\"\\n\")\n",
    "\n",
    "    res = []\n",
    "\n",
    "    for i in corpus:\n",
    "        res1 = []\n",
    "        for j in i:\n",
    "            if j.lower() not in stop_words and j not in LISTA_NO_ALFANUMERICOS and len(j) > 1:\n",
    "                res1.append(j.lower())\n",
    "    res.append(res1)\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta función podremos observar como realizamos la tokenización de las palabras, es decir, adaptar los textos para que se conviertan en vectores de palabras que después nos servirán para entrenar el modelo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación y entrenamiento del modelo:\n",
    "A continuación realizaremos las funciones que se encargarán de leer los archivos alojados en la carpeta corpus, de tal manera dichos archivos se añadirán al corpus y se guardarán con su título, el cual representa la temática\n",
    "\n",
    "Por ejemplo:\n",
    "`filosofía.txt` - archivo que contiene un texto con temática filosófica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text_to_corpus(corpus,tematica,tematicas):\n",
    "    new_corpus = tokenizar(\"corpus/\"+tematica+\".txt\")\n",
    "    corpus += new_corpus\n",
    "    tematicas.append(tematica)\n",
    "    return corpus, tematicas\n",
    "\n",
    "\n",
    "def train_word2vec(corpus):\n",
    "    model = Word2Vec(\n",
    "        corpus,\n",
    "        vector_size=100,\n",
    "        window=5,\n",
    "        min_count=1,\n",
    "        workers=4\n",
    "    )\n",
    "    model.train(corpus, total_examples=len(corpus), epochs=model.epochs)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta manera solo nos hará falta llamar a las funciones en el método main para evitar grandes volúmenes de código y hacer este mucho mas legible e intuitibo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de *Embeddings*\n",
    "Los embeddings son los vectores de palabras que se crean a partir del corpus de entrenamiento y el modelo, estos representan la semántica de las palabras en función de su contexto en el corpus y se genera un vector de números reales.\n",
    "Estos embeddings nos permiten determinar la diferencia entre palabras y encontrar palabras mas o menos similares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embeddings(corpus, vectores):\n",
    "    corpus_embeddings = []\n",
    "    for c in corpus:\n",
    "        # Obtener el embedding de cada palabra en la frase y promediarlos\n",
    "        embeddings = [vectores[word] for word in c if word in vectores]\n",
    "        if embeddings:\n",
    "            sentence_embedding = numpy.mean(embeddings, axis=0)\n",
    "            corpus_embeddings.append(sentence_embedding)\n",
    "    return corpus_embeddings\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este fragmento del codigo se devolverán los emedings pasándole como parámetros el corpues (puede ser entrenamiento o test) y el modelo entrenado."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función main:\n",
    "Es aquí donde se realizan las llamadas a función y se ejecuta finalmente el programa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Carpeta donde se encuentran los archivos de texto\n",
    "    folder_path = \"corpus\"\n",
    "\n",
    "    # Corpus inicial y temáticas\n",
    "    corpus = []\n",
    "    tematicas = []\n",
    "\n",
    "    # Añadir textos al corpus\n",
    "    for filename in os.listdir(folder_path):\n",
    "      if filename.endswith(\".txt\"):\n",
    "        tematica = filename.split(\".txt\")[0]\n",
    "        corpus, tematicas = add_text_to_corpus(corpus, tematica, tematicas)\n",
    "\n",
    "\n",
    "    # Entrenar el modelo Word2Vec\n",
    "    model = train_word2vec(corpus)\n",
    "\n",
    "    # Obtener representaciones de embeddings\n",
    "    test_file = \"test/test3.txt\"\n",
    "    test_corpus = tokenizar(test_file)\n",
    "    test_embeddings = embeddings(test_corpus, model.wv)\n",
    "    total_embeddings = embeddings(corpus, model.wv)\n",
    "\n",
    "    # Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "    X_train = total_embeddings\n",
    "    y_train = tematicas\n",
    "\n",
    "    # Entrenar un modelo de clasificación\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Realizar predicciones en el conjunto de prueba\n",
    "    resultado_clasificacion = model.predict(test_embeddings)\n",
    "    print(\"El resultado de la clasificación fue: \" + resultado_clasificacion[0])\n",
    "\n",
    "    # Evaluar el rendimiento del modelo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar como hemos instanciado algunas variables como corpus o tematicas que usaremos durante la ejecución y que incluse se utilizan de forma recursiva para modificar sus valores.\n",
    "Hay que tener en cuenta que el texto test no se ha configurado para su lectura automática debido a que puede haber mas de un texto en la carpeta, por lo que debe ajustarse de forma manual cual de los textos de prueba se quiere clasificar."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ejecutar el código completo solamente debemos llamar a la función `main`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El resultado de la clasificación fue: fotografía\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
